# Workflow name displayed in the GitHub Actions UI.
name: Job Scraper CI/CD

# Define triggers for this workflow.
on:
  # Trigger on pushes to the 'main' and 'docker' branches for continuous integration (CI) and immediate deployment checks.
  push:
    branches:
      - main
      - docker
  # Schedule recurring runs for automated job scraping.
  # '0 2 * * *' means 08:00 AM UTC daily. Adjust cron expression as needed.
  schedule:
    - cron: '0 8 * * *'
  # Allow manual triggering from the GitHub Actions UI.
  # Includes an input for enabling debug logging, useful for troubleshooting specific runs.
  workflow_dispatch:
    inputs:
      debug_enabled:
        description: 'Enable verbose debug logging for this run?'
        required: false
        type: boolean
        default: false

# Define the jobs to be executed in this workflow.
jobs:
  scrape_and_notify:
    # Specify the runner environment. ubuntu-latest is a standard choice for Linux-based containers.
    runs-on: ubuntu-latest
    permissions:
      actions: read # Needed to download artifacts from previous runs
    # Define environment variables available to all steps in this job.
    env:
      # Map the workflow_dispatch input to an environment variable consumed by the Python script.
      DEBUG_ENABLED: ${{ github.event.inputs.debug_enabled }}

    steps:
      - name: Checkout repository code
        # Uses a standard action to clone the repository, making code available to subsequent steps.
        uses: actions/checkout@v4

      - name: Download Previous Posted Jobs Artifact
        uses: actions/github-script@v6
        env:
          WORKFLOW_FILENAME: scraper.yml
          ARTIFACT_NAME: posted-jobs-file
          ARTIFACT_FILENAME: posted_jobs.zip
          UNZIP_DIR: .
        with:
          script: |
            const fs = require('fs');
            const { execSync } = require('child_process');

            const { owner, repo } = context.repo;
            const workflowFilename = process.env.WORKFLOW_FILENAME || 'scraper.yml';
            const artifactName = process.env.ARTIFACT_NAME || 'posted-jobs-file';
            const artifactFilename = process.env.ARTIFACT_FILENAME || 'posted_jobs.zip';
            const unzipDir = process.env.UNZIP_DIR || '.';

            core.info(`Looking up workflow by filename: ${workflowFilename}`);
            const { data: workflow } = await github.rest.actions.getWorkflow({
              owner,
              repo,
              workflow_id: workflowFilename,
            });

            const currentRunId = context.runId;
            const currentBranch = context.ref.replace('refs/heads/', '');
            core.info(`Current run id: ${currentRunId}, branch: ${currentBranch}`);

            const { data: runs } = await github.rest.actions.listWorkflowRuns({
              owner,
              repo,
              workflow_id: workflow.id,
              per_page: 50,
              branch: currentBranch,
            });

            const previousSuccessfulRun = runs.workflow_runs.find((r) => r.id !== currentRunId && r.conclusion === 'success');
            if (!previousSuccessfulRun) {
              core.warning('No previous successful workflow run found on this branch; skipping artifact download.');
              return;
            }
            core.info(`Previous successful run id: ${previousSuccessfulRun.id}`);

            const { data: artifacts } = await github.rest.actions.listWorkflowRunArtifacts({
              owner,
              repo,
              run_id: previousSuccessfulRun.id,
            });

            const target = artifacts.artifacts.find((a) => a.name === artifactName && !a.expired);
            if (!target) {
              core.warning(`Artifact '${artifactName}' not found for run ${previousSuccessfulRun.id} or it is expired.`);
              return;
            }
            core.info(`Found artifact '${artifactName}' (id: ${target.id}). Downloading...`);

            const { data: archive } = await github.rest.actions.downloadArtifact({
              owner,
              repo,
              artifact_id: target.id,
              archive_format: 'zip',
            });

            fs.writeFileSync(artifactFilename, Buffer.from(archive));
            core.info(`Saved artifact to ${artifactFilename}. Extracting...`);

            // Ensure unzip is available and extract the archive
            execSync(`unzip -o ${artifactFilename} -d ${unzipDir}`);
            core.info('Artifact downloaded and extracted successfully.');

      - name: Create or Update Posted Jobs File on Host
        # Ensure the posted_jobs.txt exists on the host for Docker cp to use.
        # It will be overwritten if the artifact was downloaded.
        run: touch posted_jobs.txt

      - name: GitGuardian scan for secrets
        uses: GitGuardian/ggshield-action@v1
        env:
          GITGUARDIAN_API_KEY: ${{ secrets.GITGUARDIAN_API_KEY }}

      # --- NEW STEPS FOR CODE QUALITY CHECKS ---
      - name: Set up Python for formatting/linting checks
        uses: actions/setup-python@v5
        with:
          python-version: '3.10' # Specify your exact Python version, e.g., '3.9', '3.10', '3.11', '3.12'

      - name: Install Black for formatting check
        run: pip install black

      - name: Check code formatting with Black
        run: black . --check
        # This step will fail the workflow if the code is not formatted according to Black's rules.
        # This ensures consistent code style before building the Docker image.

      - name: Install Flake8 for linting check
        run: pip install flake8

      - name: Check code linting with Flake8
        run: flake8 .

      # Install Mypy for type checking
      - name: Install Mypy for type checking
        run: pip install mypy

      - name: Run Mypy static type checker
        run: mypy . --ignore-missing-imports
        # This step will fail the workflow if there are type checking errors in the code.
        
      # Install Pytest for running tests
      - name: Install Pytest
        run: pip install pytest

      # Install Selenium, Telegram, undetected_chromedriver, and Tenacity dependencies
      - name: Install dependencies for tests
        run: pip install selenium python-telegram-bot undetected-chromedriver tenacity pytest-asyncio

      - name: Run Pytest
        run: pytest

      - name: Cache Posted Jobs File
        # Caches the 'posted_jobs.txt' file across workflow runs. This is critical for
        # maintaining the history of sent jobs and preventing duplicate notifications.
        id: cache-posted-jobs
        uses: actions/cache@v4
        with:
          path: posted_jobs.txt # The file to cache on the runner's filesystem.
          key: ${{ runner.os }}-posted-jobs # Unique key for the cache, incorporating OS for specificity.
          restore-keys: |
            ${{ runner.os }}-posted-jobs # Ensures cache restoration even if the exact key doesn't match.

      - name: Set up Docker Buildx
        # Initializes Docker Buildx, enhancing Docker build capabilities (e.g., for multi-platform builds, caching).
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        # Authenticates with Docker Hub using securely stored GitHub Secrets.
        # This is required to push new Docker images to your repository.
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and Push Docker image to Docker Hub
        id: docker_build
        uses: docker/build-push-action@v5
        with:
          context: . # Keep the build context as the repository root
          file: docker/Dockerfile # Specify the correct path to the Dockerfile
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/devops-job-scraper:latest
            ${{ secrets.DOCKERHUB_USERNAME }}/devops-job-scraper:${{ github.sha }}
          cache-from: type=gha,scope=build
          cache-to: type=gha,scope=build,mode=max

      - name: Run Docker container for job scraping
        # Executes the newly built (and pushed) Docker image.
        # Environment variables and the persisted 'posted_jobs.txt' are mounted/passed.
        run: |
          CONTAINER_NAME="devops-job-scraper-run"
          CONTAINER_POSTED_JOBS_PATH="/app/posted_jobs.txt"
          HOST_POSTED_JOBS_FILE="${{ github.workspace }}/posted_jobs.txt"

          # Ensure no old container is lingering
          docker rm -f "${CONTAINER_NAME}" || true

          # 1. Start the container in detached mode with memory limits
          docker run -d --name ${CONTAINER_NAME} \
            --memory=2g \
            --memory-swap=2g \
            -e TELEGRAM_BOT_TOKEN="${{ secrets.TELEGRAM_BOT_TOKEN }}" \
            -e TELEGRAM_CHAT_ID="${{ secrets.TELEGRAM_CHAT_ID }}" \
            -e POSTED_JOBS_FILE="${CONTAINER_POSTED_JOBS_PATH}" \
            -e DEBUG_ENABLED="${{ env.DEBUG_ENABLED }}" \
            ${{ secrets.DOCKERHUB_USERNAME }}/devops-job-scraper:latest

          # Wait a moment for container to be fully ready
          sleep 5

          # Check if container is running
          echo "Container status:"
          docker ps -a --filter name=${CONTAINER_NAME}

          # 2. Copy the existing posted_jobs.txt from host to container
          echo "Copying posted_jobs.txt into container..."
          echo "Content of posted_jobs.txt before copying into container:"
          cat "${HOST_POSTED_JOBS_FILE}"
          echo "File size: $(wc -l < "${HOST_POSTED_JOBS_FILE}") lines"
          docker cp "${HOST_POSTED_JOBS_FILE}" "${CONTAINER_NAME}:${CONTAINER_POSTED_JOBS_PATH}"

          # Fix file permissions inside the container
          echo "Fixing file permissions inside container..."
          docker exec --user root "${CONTAINER_NAME}" chown nonroot:nonroot "${CONTAINER_POSTED_JOBS_PATH}"
          docker exec --user root "${CONTAINER_NAME}" chmod 644 "${CONTAINER_POSTED_JOBS_PATH}"

          # Debug: Show file permissions and ownership after fixing
          echo "File permissions and ownership after fixing:"
          docker exec "${CONTAINER_NAME}" ls -la "${CONTAINER_POSTED_JOBS_PATH}"

          # Debug: Show file content inside container before running Python
          echo "File content inside container before running Python:"
          docker exec "${CONTAINER_NAME}" cat "${CONTAINER_POSTED_JOBS_PATH}"
          docker exec "${CONTAINER_NAME}" sh -lc "echo \"File size inside container: \$(wc -l < '${CONTAINER_POSTED_JOBS_PATH}') lines\""

          # 3. Execute the Python application inside the running container
          echo "Running Python application in container..."
          docker exec "${CONTAINER_NAME}" python main.py

          # 4. Copy the updated posted_jobs.txt from container to host
          echo "Copying updated posted_jobs.txt from container..."
          docker cp "${CONTAINER_NAME}:${CONTAINER_POSTED_JOBS_PATH}" "${HOST_POSTED_JOBS_FILE}"

          # Debug: Show the content of the file after copying
          echo "Content of posted_jobs.txt after copying from container:"
          cat "${HOST_POSTED_JOBS_FILE}"
          echo "File size: $(wc -l < "${HOST_POSTED_JOBS_FILE}") lines"

          # 5. Stop and remove the container
          echo "Cleaning up container..."
          docker stop "${CONTAINER_NAME}"
          docker rm "${CONTAINER_NAME}"

      - name: Upload Posted Jobs File as Artifact
        # Uploads the updated 'posted_jobs.txt' file as a workflow artifact.
        # This provides a snapshot of the processed jobs for auditing and debugging,
        # irrespective of whether the job succeeded or failed (due to 'if: always()').
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: posted-jobs-file
          path: posted_jobs.txt # The file to upload from the runner's workspace.
